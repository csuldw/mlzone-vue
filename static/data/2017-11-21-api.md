## 1.保存或更新

- 接口地址：http://localhost:8080/saveOrUpdate.do
- 请求方式：POST
- 参数
	- JSON：TaskLogEntity

1.新增样例

```
{
    "ip":"192.1.11.111",
    "sss": "SSH",
    "sss":"1111111",
    "sss":"1111111",
    "sss":"cd /home",
    "detailTaskInfoEntity":{
        "ddd":"123",
        "ccc":"dog",
        "aaa":"2",
        "bbb": "11:00"
    }
}
```

```
#爬取单个用户的follow，ulr = http://weibo.cn/%uid/follow?page=1
def getUserFollows(self, uid, params="page=1"):
    time.sleep(2)   
    url = "http://weibo.cn/%s/follow?%s" %(uid, params)
    self.logger.info("page is: " + url)
    text = self.openURL(url)
    soup = BS(text, "html.parser")
    res = soup.find_all('table')
    reg_uid = r"uid=(\d+)&" #匹配uid
    follows = {"uid": uid, "follow_ids": list(set([y for x in [re.findall(reg_uid, str(elem)) for elem in res] for y in x ]))}
    next_url = re.findall('<div><a href="(.*?)">下页</a>&nbsp', text) #匹配"下页"内容
    if len(next_url) != 0:
        url_params = next_url[0].split("?")[-1] 
        follows['follow_ids'].extend(self.getUserFollows(uid, params=url_params)["follow_ids"]) #将结果集合并
    return follows
```

```
"""
@func: 获取用户的发的微博信息
"""
def getUserTweets(self, uid, tweets_all, params="page=1"):
    self.switchUserAccount(myconf.userlist)
    url = r"http://weibo.cn/%s/profile?%s" %(uid, params)
    self.logger.info("URL path is: " + url)
    text = self.openURL(url)
    soup = BS(text, "html.parser")
    res = soup.find_all("div", {"class":"c"})
    #规则：如果div中子div数量为1，则为一个原厂文本说说；数量为2，则根据cmt判断是原创图文还是转发文本说说；数量为3，则为转发图文
    tweets_list = []
    for elem in res:
        tweets = {}
        unicode_text = unicode(elem)
        sub_divs = elem.find_all("div")
        today = time.strftime('%Y-%m-%d',time.localtime(time.time()))
        if len(sub_divs) in [1, 2, 3]:
            tweets["uid"] = uid
            tweets["reason"] = "null"
            #tweets["content"] = re.findall("<span class=\"ctt\">(.*?)</span>", str(elem.find("span", {"class": "ctt"})))[0]
            tweets["content"] = elem.find("span", {"class": "ctt"}).text
            soup_text = elem.find("span", {"class": "ct"}).text
            created_at = re.findall("\d\d\d\d-\d\d-\d\d \d\d:\d\d:\d\d", unicode(soup_text))
            post_time = re.findall("\d\d:\d\d", unicode(soup_text))
            split_text = unicode(soup_text).split(u"\u5206\u949f\u524d")
            if not created_at:
                created_at = re.findall(u"\d\d\u6708\d\d\u65e5 \d\d:\d\d", unicode(soup_text))
                tweets["created_at"] = time.strftime("%Y-",time.localtime()) + unicode(created_at[0]).replace(u"\u6708", "-").replace(u"\u65e5", "") + ":00"
                tweets["source"] = soup_text.split(created_at[0])[-1].strip(u"\u00a0\u6765\u81ea")
            elif created_at:
                tweets["created_at"] = unicode(created_at[0]).replace(u"\u6708", "-").replace(u"\u65e5", "")
                tweets["source"] = soup_text.split(created_at[0])[-1].strip(u"\u00a0\u6765\u81ea")
            elif post_time:
                tweets["created_at"] = today + " " + post_time[0] + ":00"
                tweets["source"] = soup_text.split(post_time[0])[-1].strip(u"\u00a0\u6765\u81ea")
            elif len(split_text) == 2:
                tweets["created_at"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time() - int(split_text[0])*60))
                tweets["source"] = split_text[-1].strip(u"\u00a0\u6765\u81ea")
            tweets["like_count"] = re.findall(u'\u8d5e\[(\d+)\]', unicode_text)[-1]
            tweets["repost_count"] = re.findall(u'\u8f6c\u53d1\[(\d+)\]', unicode_text)[-1]
            tweets["comment_count"] = re.findall( u'\u8bc4\u8bba\[(\d+)\]', unicode_text)[-1]
        if len(sub_divs) == 0:
            pass
        elif len(sub_divs) == 1:
            #self.logger.info("text")
            tweets["type"] = "original_text"
        elif len(sub_divs) == 2:
            #self.logger.info("image")
            tweets["type"] = "original_image"
            #根据cmt的存在判断是否为转发的文字和原创的图文说说
            cmt = elem.find_all("span", {"class": "cmt"})
            if cmt: 
                tweets["type"] = "repost_text"
                tweets["reason"] = re.findall("</span>(.*?)<a", str(sub_divs[1]))[0]
        elif len(sub_divs) == 3:
            #self.logger.info("repost")
            tweets["type"] = "repost_image"
            tweets["reason"] = re.findall("</span>(.*?)<a", str(sub_divs[2]))[0]
        else:
            self.logger.error("parse error")
            pass
        if tweets:
            tweets_list.append(json.dumps(tweets))
    self.output("\n".join(tweets_list), "output/" + uid + "/" + uid + "_tweets_" + params.replace("=", "") + ".json")
    
    next_url = re.findall('<div><a href="(.*?)">下页</a>&nbsp', text) #匹配"下页"内容
    if len(next_url) != 0:
        url_params = next_url[0].split("?")[-1]
        tweets_all.extend(tweets_list)
        self.getUserTweets(uid, tweets_all, params=url_params)
    return tweets_list
```
